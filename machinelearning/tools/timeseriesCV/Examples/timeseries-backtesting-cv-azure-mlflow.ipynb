{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient\n",
        "\n",
        "try:\n",
        "    credential = DefaultAzureCredential()\n",
        "    # Check if given credential can get token successfully.\n",
        "    credential.get_token(\"https://management.azure.com/.default\")\n",
        "except Exception as ex:\n",
        "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
        "    credential = InteractiveBrowserCredential()\n",
        "\n",
        "ml_client = MLClient.from_config(credential=credential)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706654736299
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile env.yml\n",
        "name: mlflow-env\n",
        "channels:\n",
        "  - conda-forge\n",
        "dependencies:\n",
        "  - python=3.10\n",
        "  - pip\n",
        "  - pip:\n",
        "    - numpy\n",
        "    - pandas\n",
        "    - scikit-learn\n",
        "    - matplotlib\n",
        "    - mlflow\n",
        "    - azureml-mlflow\n",
        "    - pmdarima\n",
        "    - prophet\n",
        "    - stats_can\n",
        "    - openpyxl"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting env.yml\n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Environment\n",
        "\n",
        "my_env = Environment(\n",
        "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04\",\n",
        "    conda_file=\"./env.yml\",\n",
        "    name=\"time-series-backtest\",\n",
        "    description = \"Environment for time series model backtesting.\"\n",
        ")\n",
        "\n",
        "ml_client.environments.create_or_update(my_env)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "Environment({'intellectual_property': None, 'is_anonymous': False, 'auto_increment_version': False, 'auto_delete_setting': None, 'name': 'time-series-backtest', 'description': 'Environment for time series model backtesting.', 'tags': {}, 'properties': {'azureml.labels': 'latest'}, 'print_as_yaml': True, 'id': '/subscriptions/ba7684dd-6ce0-49a1-9eab-54c65c60d6e1/resourceGroups/rg-dp100-ldc63a064cfb7438089/providers/Microsoft.MachineLearningServices/workspaces/mlw-dp100-ldc63a064cfb7438089/environments/time-series-backtest/versions/4', 'Resource__source_path': None, 'base_path': '/mnt/batch/tasks/shared/LS_root/mounts/clusters/cidc63a064cfb7438089/code/Users/Gkaube', 'creation_context': <azure.ai.ml.entities._system_data.SystemData object at 0x7f4052bd2f50>, 'serialize': <msrest.serialization.Serializer object at 0x7f4052bd2e60>, 'version': '4', 'latest_version': None, 'conda_file': {'channels': ['conda-forge'], 'dependencies': ['python=3.10', 'pip', {'pip': ['numpy', 'pandas', 'scikit-learn', 'matplotlib', 'mlflow', 'azureml-mlflow', 'pmdarima', 'prophet', 'stats_can', 'openpyxl']}], 'name': 'mlflow-env'}, 'image': 'mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04', 'build': None, 'inference_config': None, 'os_type': 'Linux', 'arm_type': 'environment_version', 'conda_file_path': None, 'path': None, 'datastore': None, 'upload_hash': None, 'translated_conda_file': '{\\n  \"channels\": [\\n    \"conda-forge\"\\n  ],\\n  \"dependencies\": [\\n    \"python=3.10\",\\n    \"pip\",\\n    {\\n      \"pip\": [\\n        \"numpy\",\\n        \"pandas\",\\n        \"scikit-learn\",\\n        \"matplotlib\",\\n        \"mlflow\",\\n        \"azureml-mlflow\",\\n        \"pmdarima\",\\n        \"prophet\",\\n        \"stats_can\",\\n        \"openpyxl\"\\n      ]\\n    }\\n  ],\\n  \"name\": \"mlflow-env\"\\n}'})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706586219800
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile src/cv_test.py\n",
        "from samplemodels import ARIMAModel\n",
        "from timeseriescrossvalidation import TimeSeriesCrossValidator\n",
        "from pipeline import Pipeline\n",
        "from utils import initialize_directories\n",
        "import argparse\n",
        "import pandas as pd\n",
        "import mlflow\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def main(args):\n",
        "    initialize_directories()\n",
        "\n",
        "    path = args.input_data_path\n",
        "    order = tuple(args.order)\n",
        "    features = args.features\n",
        "    pca_comps = args.pca_comps\n",
        "    \n",
        "    run_desc = \"Back testing ARIMA{}, with {} pca components with features: {}\".format(order, pca_comps, features)\n",
        "    \n",
        "    with mlflow.start_run(description=run_desc):\n",
        "        data = load_data(path=path)\n",
        "        data_clean = prep_data(data)\n",
        "        data_clean.index = pd.DatetimeIndex(data_clean['ds'])\n",
        "        data_clean.to_csv(path + \"/trainv1.csv\")\n",
        "        mlflow.log_artifact(path + \"/trainv1.csv\", artifact_path=\"data\")\n",
        "\n",
        "        # y values that are 0.00 have just not been observed yet\n",
        "        data_clean = data_clean[data_clean['y'] != 0.00]\n",
        "        \n",
        "        \n",
        "        mlflow.log_param(\"Features used for training\", features)\n",
        "\n",
        "        cross_validation(data_clean,\n",
        "                         order = order,\n",
        "                         columns = features,\n",
        "                         pca_comps = pca_comps)\n",
        "\n",
        "        mlflow.set_tag(\"Training Info\", \"Cross validation\")\n",
        "\n",
        "        X_train, y_train = split_data(data_clean, features)\n",
        "\n",
        "        # Apply PCA to the data\n",
        "        X_transformed = dim_recuction(X_train, pca_comps)\n",
        "\n",
        "        X_transformed.to_csv(path + \"/X_transformed.csv\")\n",
        "        y_train.to_csv(path + \"/y_train.csv\")\n",
        "        mlflow.log_artifact(path + \"/X_transformed.csv\", artifact_path=\"data\")\n",
        "        mlflow.log_artifact(path + \"/y_train.csv\", artifact_path=\"data\")\n",
        "\n",
        "        trained_model = train_model(X_transformed, y_train, order)\n",
        "        # Log the model\n",
        "        log_model(trained_model, X_transformed, y_train)\n",
        "\n",
        "\n",
        "def cross_validation(data_set, \n",
        "                     order:tuple,\n",
        "                     columns:list,\n",
        "                     pca_comps:int = 0\n",
        "                    ):\n",
        "    \n",
        "    cv = TimeSeriesCrossValidator(ARIMAModel(order=order), fold_size=4, order=order, with_intercept=True)\n",
        "    \n",
        "    \n",
        "    if pca_comps > 0:\n",
        "        cv.principal_components(n_components=2)\n",
        "    \n",
        "    \n",
        "    # Subset data_set\n",
        "    train_modified = data_set[columns +['y','ds']]\n",
        "    \n",
        "    # Run the backtest and print metrics\n",
        "    cv.fit(train_modified, start=33, end = 55)\n",
        "    cv.print_metrics()\n",
        "    \n",
        "    # Log parameters and metrics\n",
        "    mlflow.log_params(cv.model.model.get_params())\n",
        "    mlflow.log_metric(\"Train MSE\", np.mean(cv.train_mse))\n",
        "    mlflow.log_metric(\"MSE\", np.mean(cv.mse))\n",
        "    mlflow.log_metric(\"RMSE\", np.mean(np.sqrt(cv.mse)))\n",
        "    mlflow.log_metric(\"MAE\", np.mean(cv.mae))\n",
        "    mlflow.log_metric(\"MAPE\", np.mean(cv.mape))\n",
        "\n",
        "    # Log training performance\n",
        "    train_mse_per_fold = {f\"Fold {i+1}\": j for i, j in enumerate(cv.train_mse)}\n",
        "    mlflow.log_dict(train_mse_per_fold, \"train_mse_per_fold.json\")\n",
        "\n",
        "    # Do some plotting\n",
        "    plot_metric_per_fold(\"RMSE\", np.sqrt(cv.mse))\n",
        "    plot_metric_per_fold(\"MAPE\", cv.mape)\n",
        "    \n",
        "\n",
        "def plot_metric_per_fold(metric_name, metric:list):\n",
        "    metric_per_fold = {f\"Fold {i+1}\": j for i, j in enumerate(metric)}\n",
        "    fold_list = metric_per_fold.keys()\n",
        "    metrics = metric_per_fold.values()\n",
        "    fig=plt.figure(figsize=(6, 4))\n",
        "    plt.bar(x=fold_list, height=metrics)\n",
        "    plt.title(f\"{metric_name} per fold\")\n",
        "    plt.ylabel(f\"{metric_name}\")\n",
        "    plt.xlabel(\"Fold\")\n",
        "    \n",
        "    plot_name = f\"{metric_name}_per_fold.png\"\n",
        "    plt.savefig(plot_name) \n",
        "    mlflow.log_artifact(plot_name)\n",
        "\n",
        "\n",
        "def load_data(path=\"./data\"):\n",
        "    data_pipeline = Pipeline(data_path=path)\n",
        "    data = data_pipeline.run()\n",
        "    return data\n",
        "\n",
        "\n",
        "def prep_data(df:pd.DataFrame):\n",
        "    df = df.dropna()\n",
        "    df.columns = map(str.lower, df.columns)\n",
        "    df.columns = df.columns.str.replace(' ', '_')\n",
        "    return df[df.sales != 0.00].reset_index(drop=True)\n",
        "\n",
        "\n",
        "def train_model(X, y, order):\n",
        "    import pmdarima as pm\n",
        "    model = pm.ARIMA(order=order, with_intercept=True)\n",
        "    model.fit(y, X)\n",
        "    return model\n",
        "    \n",
        "\n",
        "def log_model(model, X, y):\n",
        "    from mlflow.models.signature import infer_signature\n",
        "    # Get insample predictions\n",
        "    predictions = model.predict_in_sample(X)\n",
        "\n",
        "    signature = infer_signature(y, predictions)\n",
        "    mlflow.pmdarima.log_model(model, \"model\", signature=signature)\n",
        "\n",
        "\n",
        "def split_data(data, features):\n",
        "    X = data[features]\n",
        "    y = data['y']\n",
        "    return X, y\n",
        "\n",
        "\n",
        "def dim_recuction(X, n_comps=2):\n",
        "    from sklearn.decomposition import PCA\n",
        "    pca = PCA(n_components=n_comps)\n",
        "    \n",
        "    X_transformed = pca.fit_transform(X)\n",
        "    X_transformed_df = pd.DataFrame(X_transformed, index=pd.to_datetime(X.index))\n",
        "\n",
        "    return X_transformed_df\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    \n",
        "    parser.add_argument(\"--data_path\", dest = \"input_data_path\", default = \"./data\", type=str)\n",
        "    parser.add_argument(\"--order\", action=\"append\", dest = \"order\", type=int)\n",
        "    parser.add_argument(\"--features\", action=\"append\", dest = \"features\")\n",
        "    parser.add_argument(\"--pca_comps\", dest = \"pca_comps\", type=int, default=2)\n",
        "    \n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    main(args)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Overwriting src/cv_test.py\n"
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import command\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "\n",
        "inputs = {\n",
        "    \"data_path\": Input(type=AssetTypes.URI_FOLDER, path=\"train-data:version\", mode= 'download'),\n",
        "    \"ar\": 3,\n",
        "    \"I\": 1,\n",
        "    \"ma\": 0,\n",
        "    \"feature1\": \"feature1\", \n",
        "    \"feature2\":\"feature2\"\n",
        "}\n",
        "\n",
        "job = command(\n",
        "    code = \"./src\",\n",
        "    command = \"python cv_test.py --data_path ${{inputs.data_path}} --order ${{inputs.ar}} --order ${{inputs.I}} --order ${{inputs.ma}} --features ${{inputs.feature1}} --features ${{inputs.feature2}}\",\n",
        "    inputs = inputs,\n",
        "    environment=\"time-series-backtest:4\",\n",
        "    compute = \"aml-cluster\",\n",
        "    display_name= \"ts-cv\",\n",
        "    experiment_name=\"timeseries-cv\"\n",
        ")\n",
        "\n",
        "job_results = ml_client.create_or_update(job)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "\u001b[32mUploading src (0.05 MBs): 100%|██████████| 51818/51818 [00:00<00:00, 376405.10it/s]\n\u001b[39m\n\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706656523973
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml.entities import Model\n",
        "job_name = job_results.name\n",
        "print(job_name)\n",
        "\n",
        "# Register the model\n",
        "saved_model = Model(\n",
        "    path=f\"azureml://jobs/{job_name}/outputs/artifacts/paths/model/\",\n",
        "    name = \"Arima_3_1_0\",\n",
        "    description= \"Arima(3,1,0) trained with pca features\",\n",
        "    type=AssetTypes.MLFLOW_MODEL,\n",
        ")\n",
        "\n",
        "ml_client.models.create_or_update(saved_model)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "musing_arch_sgkgrz8v7r\n"
        },
        {
          "output_type": "error",
          "ename": "ResourceExistsError",
          "evalue": "(UserError) Conflict\nCode: UserError\nMessage: Conflict\nException Details:\t(ModelVersionInUse) Model Arima_3_1_0:2 already exists. Please use a different name or version.\n\tCode: ModelVersionInUse\n\tMessage: Model Arima_3_1_0:2 already exists. Please use a different name or version.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExistsError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Register the model\u001b[39;00m\n\u001b[1;32m      6\u001b[0m saved_model \u001b[38;5;241m=\u001b[39m Model(\n\u001b[1;32m      7\u001b[0m     path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mazureml://jobs/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/outputs/artifacts/paths/model/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArima_3_1_0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      9\u001b[0m     description\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArima(3,1,0) trained with pca features\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mAssetTypes\u001b[38;5;241m.\u001b[39mMLFLOW_MODEL,\n\u001b[1;32m     11\u001b[0m )\n\u001b[0;32m---> 13\u001b[0m \u001b[43mml_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_telemetry/activity.py:263\u001b[0m, in \u001b[0;36mmonitor_with_activity.<locals>.monitor.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m    261\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m log_activity(logger, activity_name \u001b[38;5;129;01mor\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, activity_type, custom_dimensions):\n\u001b[0;32m--> 263\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:235\u001b[0m, in \u001b[0;36mModelOperations.create_or_update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    233\u001b[0m     log_and_raise_error(ex)\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 235\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ex\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:223\u001b[0m, in \u001b[0;36mModelOperations.create_or_update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m ASSET_PATH_ERROR:\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m AssetPathException(\n\u001b[1;32m    218\u001b[0m             message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG,\n\u001b[1;32m    219\u001b[0m             target\u001b[38;5;241m=\u001b[39mErrorTarget\u001b[38;5;241m.\u001b[39mMODEL,\n\u001b[1;32m    220\u001b[0m             no_personal_data_message\u001b[38;5;241m=\u001b[39mCHANGED_ASSET_PATH_MSG_NO_PERSONAL_DATA,\n\u001b[1;32m    221\u001b[0m             error_category\u001b[38;5;241m=\u001b[39mErrorCategory\u001b[38;5;241m.\u001b[39mUSER_ERROR,\n\u001b[1;32m    222\u001b[0m         )\n\u001b[0;32m--> 223\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    225\u001b[0m model \u001b[38;5;241m=\u001b[39m Model\u001b[38;5;241m.\u001b[39m_from_rest_object(result)\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m auto_increment_version \u001b[38;5;129;01mand\u001b[39;00m indicator_file:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/operations/_model_operations.py:202\u001b[0m, in \u001b[0;36mModelOperations.create_or_update\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    191\u001b[0m auto_increment_version \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_auto_increment_version\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     result \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_versions_operation\u001b[38;5;241m.\u001b[39mbegin_create_or_update(\n\u001b[1;32m    195\u001b[0m             name\u001b[38;5;241m=\u001b[39mname,\n\u001b[1;32m    196\u001b[0m             version\u001b[38;5;241m=\u001b[39mversion,\n\u001b[1;32m    197\u001b[0m             body\u001b[38;5;241m=\u001b[39mmodel_version_resource,\n\u001b[1;32m    198\u001b[0m             registry_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name,\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scope_kwargs,\n\u001b[1;32m    200\u001b[0m         )\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    201\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model_versions_operation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_or_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m            \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m            \u001b[49m\u001b[43mversion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_version_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m            \u001b[49m\u001b[43mworkspace_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_workspace_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scope_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[1;32m    211\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_registry_name:\n\u001b[1;32m    212\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get(name\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mname, version\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mversion)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/tracing/decorator.py:76\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/ai/ml/_restclient/v2023_04_01_preview/operations/_model_versions_operations.py:648\u001b[0m, in \u001b[0;36mModelVersionsOperations.create_or_update\u001b[0;34m(self, resource_group_name, workspace_name, name, version, body, **kwargs)\u001b[0m\n\u001b[1;32m    645\u001b[0m response \u001b[38;5;241m=\u001b[39m pipeline_response\u001b[38;5;241m.\u001b[39mhttp_response\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m200\u001b[39m, \u001b[38;5;241m201\u001b[39m]:\n\u001b[0;32m--> 648\u001b[0m     \u001b[43mmap_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatus_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstatus_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    649\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mErrorResponse, pipeline_response)\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror, error_format\u001b[38;5;241m=\u001b[39mARMErrorFormat)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/azure/core/exceptions.py:109\u001b[0m, in \u001b[0;36mmap_error\u001b[0;34m(status_code, response, error_map)\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    108\u001b[0m error \u001b[38;5;241m=\u001b[39m error_type(response\u001b[38;5;241m=\u001b[39mresponse)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m error\n",
            "\u001b[0;31mResourceExistsError\u001b[0m: (UserError) Conflict\nCode: UserError\nMessage: Conflict\nException Details:\t(ModelVersionInUse) Model Arima_3_1_0:2 already exists. Please use a different name or version.\n\tCode: ModelVersionInUse\n\tMessage: Model Arima_3_1_0:2 already exists. Please use a different name or version."
          ]
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1706656136901
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}